{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPyZUcrUv21PQa3LRtqkj2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiharshith426/Project-Tasks/blob/main/Sitafal_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 langchain langchain-community faiss-cpu transformers\n"
      ],
      "metadata": {
        "id": "lFMC4SJeGG0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from transformers import pipeline\n",
        "import os\n",
        "\n",
        "# Configure Hugging Face API token for embeddings\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"Hugging_face_API_KEY\"\n",
        "\n",
        "# Function to fetch website content\n",
        "def fetch_website_content(url):\n",
        "    response = requests.get(url)\n",
        "    parsed_html = BeautifulSoup(response.text, 'html.parser')\n",
        "    text_data = parsed_html.get_text()  # Extract the raw text content\n",
        "    return text_data\n",
        "\n",
        "# Collect content from websites\n",
        "web_pages = [\n",
        "    {\"content\": fetch_website_content(\"https://www.uchicago.edu/\"), \"source\": \"University of Chicago\"},\n",
        "    {\"content\": fetch_website_content(\"https://www.washington.edu/\"), \"source\": \"University of Washington\"},\n",
        "    {\"content\": fetch_website_content(\"https://www.stanford.edu/\"), \"source\": \"Stanford University\"},\n",
        "    {\"content\": fetch_website_content(\"https://und.edu/\"), \"source\": \"University of North Dakota\"}\n",
        "]\n",
        "\n",
        "# Break the content into smaller sections\n",
        "content_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "split_content_list = []\n",
        "for page in web_pages:\n",
        "    sections = content_splitter.split_text(page['content'])\n",
        "    for section in sections:\n",
        "        split_content_list.append(Document(page_content=section, metadata={\"source\": page['source']}))\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = HuggingFaceBgeEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# Build the vector database\n",
        "vector_database = FAISS.from_documents(split_content_list, embedding_model)\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "text_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Get the query from the user\n",
        "user_query = input(\"Please enter your query: \")\n",
        "\n",
        "# Search for relevant sections\n",
        "retrieved_documents = vector_database.similarity_search(user_query, k=3)\n",
        "\n",
        "# Combine and summarize the relevant content\n",
        "combined_content = \" \".join([doc.page_content for doc in retrieved_documents])\n",
        "result_summary = text_summarizer(combined_content, max_length=150, min_length=50, do_sample=False)\n",
        "\n",
        "# Display the final summary\n",
        "print(\"\\nResponse Summary:\")\n",
        "print(result_summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "id": "W0jsEfhOLmvG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}